# Rust Test Harness Design (No Code Yet)

This document specifies the Rust-based load generator and probe for stress-testing a Zenoh cluster. It defines roles, CLI, data formats, concurrency, metrics, and Docker integration without adding any implementation code.

## Goals

- One binary providing roles: publisher, subscriber, requester, queryable.
- Deterministic, repeatable workloads with open-loop rate control.
- Accurate latency metrics (time-to-first/last) and throughput, with CSV outputs and optional Prometheus.
- Clean integration with Docker Compose and static, explicit endpoints (no auto discovery).

## High-level architecture

- Single crate, single binary with subcommands via `clap`.
- Async runtime: `tokio`.
- Zenoh client library for pub/sub and query APIs.
- Internal modules:
  - `roles::publisher`
  - `roles::subscriber`
  - `roles::requester`
  - `roles::queryable`
  - `payload` (message header encode/decode)
  - `rate` (rate limiter, open-loop scheduler)
  - `metrics` (counters, histograms, Prometheus exporter)
  - `output` (CSV writers, summary JSON)
  - `time_sync` (optional clock offset estimator)
  - `config` (env/CLI/file merging)
  - `logging` (tracing setup)

No module code is provided hereâ€”this is a design map.

## Binary and CLI (planned)

Binary name: `mq-bench`

Top-level flags:
- `--run-id <string>`: tag outputs; default is autogenerated timestamp.
- `--out-dir <path>`: directory for CSV/JSON outputs; default `./artifacts`.
- `--prom-port <port>`: expose Prometheus endpoint (optional; disabled by default).
- `--log-level <level>`: trace|debug|info|warn|error.
- `--config <FILE>`: optional YAML/TOML; CLI overrides file.

Subcommands and key options:

1) `pub`
- `--endpoint <tcp://host:port>` (repeatable)
- `--topic-prefix <string>` (default: `bench/topic`)
- `--topics <N>` topic count (default: 1)
- `--publishers <N>` logical publisher workers (default: 1)
- `--payload <bytes>` (100, 1024, 10240, ...)
- `--rate <msg/s>` per publisher (open-loop)
- `--duration <seconds>`
- `--reliability <best|reliable>` (as supported by zenoh)
- `--csv`

2) `sub`
- `--endpoint <tcp://host:port>`
- `--expr <key-expr>` (e.g., `bench/**`)
- `--subscribers <N>` parallel subscribers
- `--reliability <best|reliable>`
- `--csv`

3) `req`
- `--endpoint <tcp://host:port>`
- `--key-expr <expr>`
- `--qps <qps>` open-loop query rate per requester
- `--concurrency <inflight>`
- `--timeout <ms>` per query
- `--agg-window <ms>` to gather multi-replies
- `--duration <seconds>`
- `--csv`

4) `qry`
- `--endpoint <tcp://host:port>`
- `--serve-prefix <prefix>` (repeatable)
- `--reply-size <bytes>`
- `--proc-delay <ms>` artificial processing delay (+ jitter option)
- `--reliability <best|reliable>`
- `--csv`

Env mapping: any flag may be provided via `MQ_*` env vars (e.g., `MQ_ENDPOINTS=...`).

## Message format (payload)

- Payload contains a compact header followed by body bytes.
- Header fields:
  - `version` (u8)
  - `run_id` (u64 hash or UUID-short) for cross-role correlation
  - `seq` (u64) monotonically increasing per logical publisher
  - `sent_ns` (u64) monotonic send timestamp (nanoseconds)
  - `topic_id` (u32) optional identifier for topic selection
- Body length = `payload` flag minus header size.

Subscriber behavior:
- On message receive, record `recv_ns` and compute `latency_ns = recv_ns - (sent_ns + offset_estimate)`.
- Track gaps in `seq` per (publisher, topic_id) to infer loss; flag duplicates.

## Rate control (open-loop scheduler)

- Use a precise async scheduler to target exact mean rate.
- Implementation approach:
  - Compute period `T = 1e9 / rate` (ns).
  - Use `tokio::time::sleep_until(next_instant)`; update `next_instant += T`.
  - If behind by > 1T, skip forward (drop bursts) to avoid queue buildup.
- Support per-publisher independent schedulers to avoid lockstep.

## Metrics and outputs

- 1-second snapshots (configurable):
  - `sent_msgs`, `sent_bytes` (publisher)
  - `recv_msgs`, `recv_bytes` (subscriber)
  - `latency_{p50,p90,p95,p99,max}` (subscriber)
  - `loss`, `dupes`, `out_of_order`
  - For queries: `qps`, `ttf` (time-to-first), `ttl` (time-to-last), `timeouts`, `errors`, `replies_per_query`, `result_bytes`
- CSV per role:
  - Columns: `t, metric_name, value, tags...` (or wide format per snapshot)
- Summary JSON per run: parameters, aggregates, min/max, knee detection notes.

Optional Prometheus exporter:
- Histograms: latency (pub/sub), ttf/ttl (req/qr).
- Counters: sent/recv, timeouts, errors.
- Gauges: in-flight queries, backlog indicators.

## Time sync and offsets

- Default: assume same-host or NTP-synchronized clocks.
- Optional offset estimator: requester/subscriber exchanges ping-pong with peer to estimate offset and RTT; use half-RTT as baseline for offset correction.
- Allow disabling offset correction via flag.

## Reliability

- Expose `--reliability` flag; map to Zenoh session/config as supported by the chosen Zenoh version/build.
- Document that best-effort may drop under load; reliable aims for zero loss with potential latency trade-offs.

## Concurrency model

- Each role runs N workers (publishers, subscribers, requesters) under a single `tokio` runtime.
- Worker responsibilities:
  - Create session and resources (pub/sub or query)
  - Run scheduler/receive loop
  - Update atomic counters and histograms
- A central aggregator task collects per-worker deltas every snapshot interval and writes CSV.

## Error handling and termination

- Fail-fast on invalid configuration (clear error messages with suggestions).
- During runtime: record errors; continue if possible; include error counts in summary.
- Termination triggers:
  - Reached `duration`
  - SIGINT/SIGTERM: flush and exit with code 130.
- On exit: print one-line summary and path to artifacts.

## Configuration resolution

- Priority: CLI > env vars > config file defaults.
- Config file supports YAML or TOML; only a subset is needed to start (endpoints, rates, sizes, duration).

## File and module layout (planned)

```
src/
  main.rs                 # clap CLI, subcommand dispatch
  roles/
    mod.rs
    publisher.rs
    subscriber.rs
    requester.rs
    queryable.rs
  payload.rs              # header encode/decode helpers
  rate.rs                 # open-loop scheduler
  metrics/
    mod.rs
    stats.rs              # hdrhistogram summaries, rolling windows
    prometheus.rs         # optional exporter
  output.rs               # CSV and summary JSON
  time_sync.rs            # optional offset estimator
  config.rs               # config load/merge
  logging.rs              # tracing setup
```

## Dependencies (to be added later in Cargo.toml)

- Core: `tokio`, `clap`, `anyhow` or `thiserror`, `serde` + `serde_(json|yaml)`
- Metrics: `hdrhistogram`, `prometheus` (optional), `csv`, `chrono`
- Zenoh: `zenoh` crate (pin version)
- Logging: `tracing`, `tracing-subscriber`
- System: `sysinfo` (optional resource snapshots)

## Docker integration

- The binary will read endpoints and role parameters from CLI/env; suitable for Compose service args.
- Output directory is mounted volume (`./artifacts:/artifacts`) so CSV/JSON persist.
- Healthcheck: optional `/health` on the Prometheus HTTP server.

## Testing plan (later)

- Unit tests: payload header encode/decode; rate scheduler catch-up logic; histogram summaries.
- Integration (local): in-process publisher+subscriber with loopback Zenoh session.
- Integration (Docker): compose profile stands up routers; run `pub`+`sub` for smoke; `req`+`qry` basic.

## Non-goals (initially)

- Complex coordinator service; external databases; heavy UIs.
- Realistic application payloads beyond fixed-size blobs.

## Open questions

- Which Zenoh version/tag to pin?
- Do we enable Prometheus by default or CSV-only for first runs?
- Any must-have metrics beyond those listed?
